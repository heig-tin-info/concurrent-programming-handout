\chapter{Verrous et sémaphores}
\startchapter

\lettrine[lines=4]{L}{es} solutions au problème de l'exclusion mutuelle présentées jusqu'à présent sont difficiles à réaliser et à comprendre car il n'est jamais clair si une variable est utilisée pour implémenter l'exclusion mutuelle où si elle est utilisée pour autre chose.
De plus ces solutions sont basées sur l'attente active~:  une tâche qui ne peut pas progresser dans sa partie prélude occupe inutilement le processeur.  Un processeur peut être utilisé d'une façon plus productive s'il n'attend pas activement qu'une condition change d'état.

\section{Verrou}\label{verrou:intro}
Les {\em verrous} ont été introduits pour résoudre (partiellement) ces problèmes. Un verrou $v$ est une variable booléenne sur laquelle deux opérations atomiques sont définies, $Verrouille(v)$ et $D\acute{e}verrouille(v)$.  Ce sont les deux seules opérations (mis à part la création et sa destruction) qui peuvent manipuler le verrou.
Les primitives $Verrouille(v)$ et $D\acute{e}verrouille(v)$ sont équivalentes à l'exécution atomique des séquences d'instructions données ci-dessous.

\centering
\begin{tabular}{l}
\lstset{language=C++}
\begin{lstlisting}
void Verrouille(verrou v)
{
	if (v)
		v = false;
	else
		suspendre la tâche appelante dans la file associée à v
}

void Déverrouille(verrou v)
{
	if (la file associée à v != vide)
		débloquer une tâche en attente dans file
	else
		v = true;
}
\end{lstlisting}
\end{tabular}

\par
Ainsi, si un verrou $v$ est initialisé à vrai et qu'une tâche appelle $Verrouille(v)$, le verrou est positionné à faux. Si une seconde tâche appelle $Verrouille(v)$, alors cette tâche, qui est à l'état élu, passe à l'état bloqué et joint la file d'attente associée à $v$.
La primitive $D\acute{e}verrouille(v)$ réalise l'opération inverse. S'il y a une tâche en attente sur le verrou $v$, alors cette tâche est réactivée. La tâche réveillée sort de la file d'attente associée à $v$ pour joindre la file des tâches prêtes. Remarquons que l'exécution de la primitive $D\acute{e}verrouille(v)$ revient à passer le verrou $v$ à la tâche réveillée.
S'il n'y a pas de tâche en attente pour obtenir le verrou $v$, la primitive $D\acute{e}verrouille(v)$ libère le verrou $v$ en le positionnant à vrai (état initial).

\subsection{Section critique}
Comme son nom le sous-entend, un verrou permet de résoudre le problème de l'exclusion mutuelle à $n$ tâches de manière simple. Il suffit de verrouiller un verrou $v$ avant d'entrer en section, bloquant ainsi les autres tâches qui accèdent à leur section critique protégée par le même verrou $v$. La sortie d'une section critique se fait en déverrouillant $v$, ce qui libère le verrou ou réveille une tâche bloquée pour l'accès à sa section critique.
\centering
\vspace{-0.2 cm}
\begin{tabular}{l}
\hspace{0.6 cm}$\vdots$ \\
\begin{lstlisting}
Verrouille(v);
/* section critique */
Déverrouille(v);
\end{lstlisting} \\
\hspace{0.6 cm}$\vdots$
\end{tabular}


\subsection{Verrous en Posix}
Mis à part de définir un ensemble de fonctions permettant de gérer des threads, la bibliothèque Pthread contient aussi des fonctions manipulant des verrous.
Ces verrous sont prévus pour résoudre le problème de l'exclusion mutuelle. C'est pourquoi les noms de ces fonctions sont préfixés par \ccode{pthread_mutex}, où {\em mutex} est l'abréviation anglaise de {\em mutual exclusion}. Une description complète de ces fonctions est fournie à l'annexe A des notes de cours.

Il y a 2 façons pour créer un verrou. La première, et la plus simple, consiste à définir une variable globale de type \ccode{pthread_mutex_t} et de l'initialiser par le symbole \ccode{PTHREAD_MUTEX_INITIALIZER}. La seconde façon consiste à initialiser le verrou par la fonction \ccode{pthread_mutex_init()}. Cette méthode est indiquée si le verrou est créé dynamiquement, c.-à-d. quand une variable de type \ccode{pthread_mutex_t} est allouée par \ccode{malloc()}. Ces initialisations préparent le verrou pour réaliser la protection d'une section critique.

Les 2 opérations de verrouillage et de déverrouillage du verrou deviennent en Pthread des appels aux fonctions \ccode{pthread_mutex_lock} et \ccode{pthread_mutex_unlock} respectivement. Ces 2 fonctions ont exactement la sémantique des primitives que nous avons vu à la section \ref{verrou:intro}.

Il y a une troisième primitive, \ccode{pthread_mutex_trylock}, qui est un verrouillage immédiat. Si, au moment de l'appel, le verrou est déjà verrouillé, la fonction ne bloque pas l'appelant mais retourne la valeur \ccode{EBUSY}. Une tâche peut donc essayer de s'emparer d'un verrou, et si celui-ci est déjà pris, de faire autre chose.
\centering
\vspace{-0.2 cm}
\begin{tabular}{l}
\begin{lstlisting}
switch (pthread_mutex_trylock(&verrou)) {
	case 0: printf("%d prend le verrou\n",threadID); break;
	case EBUSY: printf("Le verrou est pris par un autre thread\n"); break;
	case EINVAL: printf("Le verrou est invalide\n");
  default: break;
}
\end{lstlisting}
\end{tabular}


Généralement un verrou a une existence équivalente à celle du thread principal. Mais il y a des situations où un verrou est créé pour accomplir une action précise et cesse d'être utile par la suite. Pour ce cas, Pthread met à disposition une fonction permettant de libérer toutes les ressources associées au verrou. Cette fonction s'appelle \ccode{pthread_mutex_destroy}.

\subsection{Coordination de tâches}
Les verrous peuvent aussi servir à résoudre les problèmes liés à la coordination (ou synchronisation) de tâches.  Par exemple, si deux tâches $T1$ et $T2$ exécutent respectivement les instructions $I_1$ et $I_2$, et que $I_1$ doit précéder l'exécution de $I_2$, la synchronisation se réalise avec un verrou $sync$ initialisé à son état verrouillé.
L'algorithme \ref{verrou:synchro1} illustre l'exemple.

\begin{algorithm}[!ht]
\caption{Coordination de 2 tâches par un verrou}\label{verrou:synchro1}
\centering
\begin{tabular}{l}
\begin{lstlisting}
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>

static pthread_mutex_t sync = PTHREAD_MUTEX_INITIALIZER;

void *T1(void *arg) {
  sleep(1);                                    // I_1
  printf("T1: fini sleep\n");
  pthread_mutex_unlock(&sync);
  return NULL;
} /* fin de T1 */

void *T2(void *arg) {
  printf("T2: avant pthread_mutex_lock\n");
  pthread_mutex_lock(&sync);
  printf("T2: apres pthread_mutex_lock\n");   // I_2
  return NULL;
} /* fin de T2 */

int main(void) {
  pthread_t tache1, tache2;
  pthread_mutex_lock(&sync);
  if (pthread_create(&tache1,NULL,T1,NULL) == 0) {
     if (pthread_create(&tache2,NULL,T2, NULL) == 0) {
        pthread_join(tache1,NULL);
        pthread_join(tache2,NULL);
        return EXIT_SUCCESS;
     }
  }
  return EXIT_FAILURE;
} /* fin de main */
\end{lstlisting}
\end{tabular}

\end{algorithm}

Remarquons que le verrou $sync$ est initialisé pour une exclusion mutuelle et, avant de lancer l'exécution des tâches \ccode{T1} et \ccode{T2}, le verrou $sync$ est verrouillé. Ainsi, si \ccode{T2} est plus rapide que \ccode{T1}, \ccode{T2} se bloque jusqu'à ce que \ccode{T1} relâche le verrou pris par \ccode{main}.

On peut généraliser le problème précédent pour que deux tâches se synchronisent de la manière suivante. Chaque tâche dispose d'une activité qu'elle peut exécuter, suivie d'une autre activité pouvant être démarrée quand l'autre tâche a terminé sa première activité.
Schématiquement, nous avons
\centering
\vspace{-0.2 cm}
\begin{tabular}{lccc}
&{\em TâcheA} & \hspace{0.5 cm} & {\em TâcheB} \\
Premières activités &\ccode{a1}     &  & \ccode{b1} \\
Rendez-vous & & $\times$ & \\
Deuxièmes activités &\ccode{a2}     &  & \ccode{b2}
\end{tabular}
%\begin{tabular}{ccc}
%{\em TâcheA} & \hspace{0.5 cm} & {\em TâcheB} \\
%\ccode{a1}     &  & \ccode{b1} \\
%\ccode{a2}     &  & \ccode{b2}
%\end{tabular}


Les activités \ccode{a2} et \ccode{b2} ne peuvent débuter que lorsque les activités \ccode{a1} et \ccode {b1} ont terminés. Ce type de synchronisation s'appelle {\em rendez-vous}. Les 2 tâches ont un point de rencontre où aucune ne peut continuer avant que l'autre n'arrive à ce point.

\subsubsection*{Première solution}
\begin{algorithm}[t]
\caption{Première solution: Rendez-vous entre 2 tâches}\label{verrou:synchro2}
\centering
\begin{tabular}{l}
\begin{lstlisting}
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>

static pthread_mutex_t arriveA = PTHREAD_MUTEX_INITIALIZER;
static pthread_mutex_t arriveB = PTHREAD_MUTEX_INITIALIZER;

void *TacheA(void *arg) {
  a1;
  pthread_mutex_lock(&arriveB);
  pthread_mutex_unlock(&arriveA);
  a2;
  return NULL;
} /* fin de TacheA */

void *TacheB(void *arg) {
  b1;
  pthread_mutex_unlock(&arriveB);
  pthread_mutex_lock(&arriveA);
  b2;
  return NULL;
} /* fin de TacheB */

int main(void) {
  pthread_t tacheA, tacheB;
  pthread_mutex_lock(&arriveA);
  pthread_mutex_lock(&arriveB);
  if (pthread_create(&tacheA,NULL,TacheA,NULL) == 0) {
     if (pthread_create(&tacheB,NULL,TacheB,NULL) == 0) {
        pthread_join(tacheA,NULL);
        pthread_join(tacheB,NULL);
        return EXIT_SUCCESS;
     }
  }
  return EXIT_FAILURE;
} /* fin de main */
\end{lstlisting}
\end{tabular}

\end{algorithm}

Notre première solution (algorithme \ref{verrou:synchro2}) demande un nombre élevé de changements de contexte. En effet, sur un monoprocesseur ou pour une exécution asynchrone, l'une des tâches arrivera au point de rendez-vous avant l'autre. Si \ccode{TacheA} arrive la première, la tâche se bloquera sur le verrou \ccode{arriveB}, puis lorsque \ccode{TacheB} termine \ccode{b1}, \ccode{TacheB} déverrouille \ccode{arriveB} avant de se bloquer sur \ccode{arriveA}. La tâche \ccode{TacheA} peut alors reprendre son exécution et déverrouiller \ccode{arriveA} avant de progresser à son activité \ccode{a2}. Dans ce cas, il y a 3 changements de contexte. Si, au contraire \ccode{TacheB} termine \ccode{b1} et arrive au point de rendez-vous avant \ccode{TacheA}, le fait de déverrouiller \ccode{arriveB} puis attendre sur \ccode{arriveA}, permet à \ccode{TacheA} de terminer son activité \ccode{a1}, puis de poursuive à \ccode{a2} sans changement de contexte.

\subsubsection*{Deuxième solution}
\begin{algorithm}[t]
\caption{Deuxième solution: rendez-vous entre 2 tâches}\label{verrou:synchro3}
\centering
\begin{tabular}{l}
\begin{lstlisting}
...
void *TacheA(void *arg) {
  a1;
  pthread_mutex_unlock(&arriveA);
  pthread_mutex_lock(&arriveB);
  a2;
  return NULL;
} /* fin de TacheA */

void *TacheB(void *arg) {
  b1;
  pthread_mutex_unlock(&arriveB);
  pthread_mutex_lock(&arriveA);
  b2;
  return NULL;
} /* fin de TacheB */
...
\end{lstlisting}
\end{tabular}

\end{algorithm}
Notre seconde solution, donnée sur algorithme \ref{verrou:synchro3}, minimise le nombre de changements de contexte. Ici, la dernière tâche qui arrive au point de rendez-vous informe l'autre via un déverrouillage, puis peut prendre le verrou positionné par l'autre tâche avant de poursuive. C'est uniquement la première tâche qui atteint le rendez-vous qui devra subir un changement de contexte.

\section{Sémaphores}
Les {\em sémaphores} sont une généralisation des verrous. Un sémaphore $s$ est une variable entière sur laquelle deux opérations atomiques sont définies, $P(s)$ (pour tester: Proberen) et $V(s)$ (pour incrémenter: Verhogen). Comme pour un verrou, ce sont les deux seules opérations (mis à part la création et l'initialisation) qui peuvent manipuler le sémaphore. Ainsi, un sémaphore généralise les valeurs que peuvent prendre le verrou, c'est la raison pour laquelle les verrous sont souvent appelés des {\em sémaphores binaires}, car ils sont limités à 2 valeurs.
Les primitives $P(s)$ et $V(s)$ sont équivalentes à l'exécution atomique des séquences d'instructions données ci-dessous.

\centering
\begin{tabular}{l}
\begin{lstlisting}
void P(sémaphore s)
{
	s -= 1;
	if (s < 0)
		suspendre la tâche appelante dans la file associée à s
}

void V(sémaphore s)
{
	s += 1;
	if (s <= 0)
		débloquer une des tâches de la file associée à s
}
\end{lstlisting}
\end{tabular}

\par
Avant de voir comment implémenter ces opérations, regardons d'abord quelques problèmes simples résolus par les sémaphores.
Les sémaphores peuvent résoudre l'exclusion mutuelle avec $n$ tâches pouvant accéder à la ressource.
Les tâches se partagent un sémaphore $mutex$ initialisé à 1.  Le protocole pour chaque tâche devient tout simplement
\centering
\begin{tabular}{l}
\hspace{0.6 cm}$\vdots$ \\
\begin{lstlisting}
P(mutex);
/* section critique */
V(mutex);
\end{lstlisting} \\
\hspace{0.6 cm}$\vdots$
\end{tabular}

\par
Selon la valeur $v>0$ à laquelle est initialisée \ccode{mutex}, jusqu'à $v$ tâches peuvent être admises simultanément dans la section critique.  On parle dans cas de {\em section contrôlée} plutôt que de section critique.  L'exclusion mutuelle devient un cas particulier d'un problème plus général~:  limiter à $n$ le nombre de tâches pouvant se trouver ensemble dans une section.

Les sémaphores servent aussi à résoudre les problèmes liés à la coordination (ou synchronisation) de tâches.  Par exemple, si deux tâches $T_1$ et $T_2$ exécutent respectivement les instructions $I_1$ et $I_2$, et que $I_1$ doit précéder l'exécution de $I_2$, la synchronisation se réalise avec un sémaphore $sync$ initialisé à 0 comme
suit
\centering
\begin{tabular}{l}
\begin{lstlisting}
void *T1(void *arg)
{
	...
	I1;
	V(sync);
	...
}
\end{lstlisting}
\hspace{1 cm}
\begin{lstlisting}
void *T2(void *arg)
{
	...
	P(sync);
	I2;
	...
}
\end{lstlisting}
\end{tabular}


\section{Implémentation des sémaphores}
Nous avons vu qu'une file d'attente est associée à chaque sémaphore. Cette file contient toutes les tâches bloquées sur ce sémaphore.  Un sémaphore peut donc se définir de la façon suivante
\centering
\begin{tabular}{l}
\begin{lstlisting}
typedef struct {
	unsigned int valeur;
	DescripteurDeTâche *liste;
} sémaphore;
\end{lstlisting}
\end{tabular}

Lorsqu'une tâche se bloque sur un sémaphore, elle est ajoutée à la liste des tâches de ce sémaphore. Lors de l'opération $V$, une des tâches est enlevée de la liste, puis réactivée. Ceci consiste à changer l'état de tâche à {\em prêt}, puis de l'insérer dans la file des tâches éligibles pour le processeur. La file d'attente d'un sémaphore s'implémente facilement avec une liste chaînée de descripteurs de processus. Le sémaphore contient un pointeur vers la liste de descripteurs.  Chaque descripteur contient aussi un pointeur pour assurer la continuité de la liste. La gestion des insertions et des retraits est l'aspect qui rend le sémaphore équitable ou non.  La gestion peut se faire comme une pile, toutefois il y aura un risque évident de famine.  La solution la plus courante est évidemment de l'implémenter comme une file PAPS.  On pourrait envisager des priorités ou n'importe quelle discipline de service.
\par
Il est intéressant d'observer l'implémentation telle que proposée, qui utilise une valeur de sémaphore non signée. Le passage dans les valeurs négatives, qui serait observé avec la réalisation de $P$ et $V$ telle que précédemment illustrée est en fait réalisée en laissant la valeur à 0, et en comptant le nombre de threads en attente dans la liste.
\par
Nous pouvons noter que l'implémentation des verrous est très similaire à celle des sémaphores, si ce n'est que la valeur entière est remplacée par un booléen. Le reste du fonctionnement, en ce qui concerne la liste en attente, est identique.
\par
Pour garantir l'atomicité des primitives $P$ et $V$, les méthodes diffèrent selon l'architecture de l'ordinateur.
Sur une machine monoprocesseur, il suffit d'inhiber les interruptions pour interdire les commutations de tâches.  Remarquons que le problème de l'exclusion mutuelle peut aussi se résoudre en inhibant les interruptions avant d'accéder en section critique, et en permettant à nouveau les interruptions en sortant de la section critique.
Il est toutefois fortement déconseillé d'assurer l'exclusion mutuelle à l'aide d'une des solutions logicielles (attente active) que nous avons étudiées au chapitre 2.

\section{Equivalence entre verrous et sémaphores}
En apparence, un sémaphore semble plus puissant qu'un verrou. Mais que faire si la plateforme à disposition ne possède que des verrous? Il faut dans ce cas réaliser ou simuler un sémaphore en n'utilisant que des verrous. La méthode est donnée par l'algorithme \ref{semaex:verrou}. Le verrou \ccode{mutex} protège la structure partagée \ccode{Semaphore} et le verrou \ccode{attente} simule la file d'attente du sémaphore.
Les verrous sont donc aussi généraux que les sémaphores; autrement dit, il n'existe aucun problème pouvant être résolu par un sémaphore et pour lequel il n'y a pas de solution en n'utilisant que des verrous.
\begin{algorithm}
\caption{Implémentation de sémaphores par verrous}\label{semaex:verrou}
\lstset{language=C++}
\begin{lstlisting}
typedef struct {
    pthread_mutex_t mutex, attente;
    int valeur;
} Semaphore;

Semaphore *CreerSemaphore(unsigned val) {
    Semaphore *s = (Semaphore *)malloc(sizeof(Semaphore));
    if (s != NULL) {
        pthread_mutex_init(&s->mutex,NULL);
        pthread_mutex_init(&s->attente,NULL);
        pthread_mutex_lock(&s->attente);
        s->valeur = val;
    }
    return s;
}

void P(Semaphore *s) {
    pthread_mutex_lock(&s->mutex);
    s->valeur -= 1;
    if (s->valeur < 0) {
        pthread_mutex_unlock(&s->mutex);
        pthread_mutex_lock(&s->attente);
    }
    pthread_mutex_unlock(&s->mutex);
}

void V(Semaphore *s) {
    pthread_mutex_lock(&s->mutex);
    s->valeur += 1;
    if (s->valeur <= 0)
        pthread_mutex_unlock(&s->attente);
    else
        pthread_mutex_unlock(&s->mutex);
}

void DetruireSemaphore(Semaphore *s) {
    pthread_mutex_destroy(&s->mutex);
    pthread_mutex_destroy(&s->attente);
    free(s);
}
\end{lstlisting}
\end{algorithm}

%\subsection{Sémaphores Posix}

%... à faire.

\section{Exercices}

\startexercice

Comment faut-il modifier l'implémentation d'un sémaphore donnée par l'algorithme \ref{semaex:verrou} si nous voulons que les valeurs du sémaphore soient sur des entiers non signés (\ccode{unsigned int})?

\startexercice

Comment implémentez-vous un verrou si vous ne disposez que de sémaphores? La sémantique du verrou doit évidemment être préservée.

% non distribué aux étudiants

\startexercice

Commentez la différence entre les deux codes suivants:

\begin{lstlisting}
void *tache(void *) {
  pthread_mutex_t mutex=PTHREAD_MUTEX_INITIALIZER;
  pthread_mutex_lock(&mutex);
  printf("Section critique\n");
  pthread_mutex_unlock(&mutex);
}
\end{lstlisting}

\begin{lstlisting}
void *tache(void *) {
  static pthread_mutex_t mutex=PTHREAD_MUTEX_INITIALIZER;
  pthread_mutex_lock(&mutex);
  printf("Section critique\n");
  pthread_mutex_unlock(&mutex);
}
\end{lstlisting}

\startexercice

Nous désirons réaliser une application possédant 2 tâches. Le programme principal est en charge de lancer les deux tâches.

Etant donné que les tâches, une fois lancées, doivent attendre un signal du programme principal pour s'exécuter, comment résoudre le problème à l'aide de verrous?

Et à l'aide de sémaphores?
